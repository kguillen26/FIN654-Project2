---
title: "Lastname-Firstname_Project1"
author:
- Guillermo Delgado
- Katie Guillen
- Leanne Harper
- Nicolas Schoonmaker
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  html_document:
    toc: true
    toc_float: true
  word_document: default
  pdf_document: default
subtitle: 'Foreign exchange market interactions'
always_allow_html: true
---

# Setup
Using RMD from: https://github.com/wgfoote/fin-alytics/blob/master/HTML/PR02_exrates-solution.html

## GitHub
Our code is on github at: https://github.com/nschoonm/FIN654-Project2

## Disable inline preview
To disable inline preview of the Markdown file, go in RStudio, Tools > Global Options... > R Markdown > Show equations and Image previews (Never)

## Knit Notes
This must be knit as an HTML file because this contains an animated gif that can not easily be added to a PDF

## Setup chunk defaults
```{r Chunk setup}
require(knitr)

# Echo the output
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(tidy = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=36))

knitr::opts_chunk$set(size = "small")
knitr::opts_hooks$set(fig.width = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width = options$fig.height
  }
  options
})
knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
```

## Packages to install
```{r Packages and Installers}
# Install RTools
# https://cran.rstudio.com/bin/windows/Rtools/
#
# Install Tinytex
# tinytex::install_tinytex()
#
# Restart R Studio
#
# Install packages
#install.packages("dplyr")
#install.packages("rstudioapi")
#install.packages("tinytex")
#install.packages("magick")
#install.packages("plotly")
#install.packages("xts")
#install.packages("ggplot2")
#install.packages("moments")
#install.packages("matrixStats")
#install.packages("quantreg")
```

## Libraries to include
```{r Library Includes}
# The list of libraries to include
library(stats)
library(dplyr)
library(rstudioapi)
```

## Get Working Directory
Get the currenet working directory so we can run this file from anywhere. This will allow this script to work from RStudio or from the command line
```{r Get current directory}
# Get the directory so we can run this from anywhere
# Get the script directory from R when running in R
if(rstudioapi::isAvailable())
{
  print("Running in RStudio")
  script.path <- rstudioapi::getActiveDocumentContext()$path
  (script.path)
  script.dir <- dirname(script.path)
}
if(!exists("script.dir"))
{
  print("Running from command line")
  script.dir <- getSrcDirectory(function(x) {x})
}
(script.dir)
```

## Set Working Directory
Set the working directory based on where the script is
```{r Working Directory and Data setup}
# Set my working directory
# There is a "data" folder here with the files and the script
setwd(script.dir)
# Double check the working directory
getwd()
# Error check to ensure the working directory is set up and the data
# directory exists inside it.  Its required for this file
if(dir.exists(paste(getwd(),"/data", sep = "")) == FALSE) {
  stop("Data directory does not exist. Make sure the working directory
       is set using setwd() and the data folder exists in it.")
} else {
  print("Working directory and data set up correctly")
}
```

# Purpose, Process, Product

This group assignment provides practice in foreign exchange markets as well as R models of those markets. Specifically we will practice reading in data, exploring time series, estimating auto and cross correlations, and investigating volatility clustering in financial time series. We will summarize our experiences in debrief. We will pay special attention to the financial economics of exchange rates.

# Assignment

This assignment will span Live Sessions 3 and 4 (two weeks). Project 2 is due before Live Session 5. Submit into **Coursework > Assignments and Grading > Project 2 > Submission** an `RMD`  file with filename **lastname-firstname_Project2.Rmd** and a knitted PDF or html file of the same name.

1. Use headers (##), r-chunks for code, and text to build a report that addresses the two parts of this project.

2. List in the text the 'R' skills needed to complete this project.

3. Explain each of the functions (e.g., `ggplot()`) used to compute and visualize results.

4. Discuss how well did the results begin to answer the business questions posed at the beginning of each part of the project.

# Part 1

In this set we will build and explore a data set using filters and `if` and `diff` statements. We will then answer some questions using plots and a pivot table report. We will then review a function to house our approach in case we would like to run some of the same analysis on other data sets.

## Problem

Marketing and accounts receivables managers at our company continue to note we have a significant exposure to exchange rates. Our functional currency (what we report in financial statements) is in U.S. dollars (USD). 

- Our customer base is located in the United Kingdom, across the European Union, and in Japan. The exposure hits the gross revenue line of our financials. 

- Cash flow is further affected by the ebb and flow of accounts receivable components of working capital in producing and selling several products. When exchange rates are volatile, so is earnings, and more importantly, our cash flow. 

- Our company has also missed earnings forecasts for five straight quarters. 

To get a handle on exchange rate exposures we download this data set and review some basic aspects of the exchange rates. 

```{r Read data}
# Read in data
library(zoo)
library(xts)
library(ggplot2)
# Read and review a csv file from FRED
exrates <- na.omit(read.csv("data/exrates.csv", header = TRUE))
# Check the data
head(exrates)
tail(exrates)
str(exrates)
# Begin to explore the data
summary(exrates)
```

## Questions

1. What is the nature of exchange rates in general and in particular for this data set? We want to reflect the ups and downs of rate movements, known to managers as currency appreciation and depreciation. 

- We will calculate percentage changes as log returns of currency pairs. Our interest is in the ups and downs. To look at that we use `if` and `else` statements to define a new column called `direction`. We will build a data frame to house this initial analysis. 

- Using this data frame, interpret appreciation and depreciation in terms of the impact on the receipt of cash flow from customer's accounts that are denominated in other than our USD functional currency.

```{r Log differences}
# Compute log differences percent using as.matrix to force numeric type
exrates.r <- diff(log(as.matrix(exrates[, -1]))) * 100
head(exrates.r)
tail(exrates.r)
str(exrates.r)
# Create size and direction
size <- na.omit(abs(exrates.r)) # size is indicator of volatility
head(size)
# colnames(size) <- paste(colnames(size),".size", sep = "") # Teetor
# another indicator of volatility
direction <- ifelse(exrates.r > 0, 1, ifelse(exrates.r < 0, -1, 0))
# colnames(direction) <- paste(colnames(direction),".dir", sep = "")
head(direction)
# Convert into a time series object: 
# 1. Split into date and rates
dates <- as.Date(exrates$DATE[-1], "%m/%d/%Y")
values <- cbind(exrates.r, size, direction)
# for dplyr pivoting we need a data frame
exrates.df <- data.frame(dates = dates, 
                         returns = exrates.r, 
                         size = size, 
                         direction = direction)
str(exrates.df) # notice the returns.* and direction.* prefixes
# 2. Make an xts object with row names equal to the dates
#order.by=as.Date(dates, "%d/%m/%Y")))
exrates.xts <- na.omit(as.xts(values, dates))
str(exrates.xts)
exrates.zr <- na.omit(as.zooreg(exrates.xts))
str(exrates.zr)
head(exrates.xts)
```

We can plot with the `ggplot2` package. In the `ggplot` statements we use `aes`, "aesthetics", to pick `x` (horizontal) and `y` (vertical) axes. Use `group =1` to ensure that all data is plotted. The added (`+`) `geom_line` is the geometrical method that builds the line plot.

```{r Exchange Rate Percent plot}
library(ggplot2)
library(plotly)
title.chg <- "Exchange Rate Percent Changes"
p1 <- autoplot.zoo(exrates.xts[,1:4]) + ggtitle(title.chg) + ylim(-5, 5)
p2 <- autoplot.zoo(exrates.xts[,5:8]) + ggtitle(title.chg) + ylim(-5, 5)
ggplotly(p1)
``` 

2. Let's dig deeper and compute mean, standard deviation, etc. Load the `data_moments()` function. Run the function using the `exrates` data and write a `knitr::kable()` report.

```{r Data moments}
#windows()
acf(coredata(exrates.xts[ , 1:4])) # returns
acf(coredata(exrates.xts[ , 5:8])) # sizes
pacf(coredata(exrates.xts[ , 1:4])) # returns
pacf(coredata(exrates.xts[ , 5:8])) # sizes
# Load the data_moments() function
## data_moments function
## INPUTS: r vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
  library(moments)
  library(matrixStats)
  mean.r <- colMeans(data)
  median.r <- colMedians(data)
  sd.r <- colSds(data)
  IQR.r <- colIQRs(data)
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  result <- data.frame(mean = mean.r, 
                       median = median.r, 
                       std_dev = sd.r, 
                       IQR = IQR.r, 
                       skewness = skewness.r, 
                       kurtosis = kurtosis.r)
  return(result)
}
# Run data_moments()
answer <- data_moments(exrates.xts[, 5:8])
# Build pretty table
answer <- round(answer, 4)
knitr::kable(answer)
mean(exrates.xts[,4])
```

# Part 2

We will use the data from the first part to investigate the interactions of the distribution of exchange rates.

## Problem

We want to characterize the distribution of up and down movements visually. Also we would like to repeat the analysis periodically for inclusion in management reports.

## Questions 
1. How can we show the shape of our exposure to euros, especially given our tolerance for risk? Suppose corporate policy set tolerance at 95\%. Let's use the `exrates.df` data frame with `ggplot2` and the cumulative relative frequency function `stat_ecdf`.

```{r Tolerable Rate Plot}
exrates.tol.pct <- 0.95
exrates.tol <- quantile(exrates.df$returns.USD.EUR, exrates.tol.pct)
exrates.tol.label <- paste("Tolerable Rate = ", round(exrates.tol, 2), "%", sep = "")
p <- ggplot(exrates.df, aes(returns.USD.EUR, fill = direction.USD.EUR)) +
  stat_ecdf(colour = "blue", size = 0.75, geom = "point") + 
  geom_vline(xintercept = exrates.tol, colour = "red", size = 1.5) +
  annotate("text", 
           x = exrates.tol + 1 , 
           y = 0.75, 
           label = exrates.tol.label, 
           colour = "darkred")
ggplotly(p)
```

2. What is the history of correlations in the exchange rate markets? If this is a "history," then we have to manage the risk that conducting business in one country will definitely affect business in another. Further that bad things will be followed by more bad things more often than good things. We will create a rolling correlation function, `corr_rolling`, and embed this function into the `rollapply()` function (look this one up!).

```{r Cross Correlation of Returns}
one <- ts(exrates.df$returns.USD.EUR)
two <- ts(exrates.df$returns.USD.GBP)
# or
one <- ts(exrates.zr[,1])
two <- ts(exrates.zr[,2])
ccf(abs(one), 
    abs(two), 
    main = "GBP vs. EUR", 
    lag.max = 20, 
    xlab = "", 
    ylab = "", 
    ci.col = "red")
# build function to repeat these routines
run_ccf <- function(one, two, main = "one vs. two", lag = 20, color = "red"){
  # one and two are equal length series
  # main is title
  # lag is number of lags in cross-correlation
  # color is color of dashed confidence interval bounds
  stopifnot(length(one) == length(two))
  one <- ts(one)
  two <- ts(two)
  main <- main
  lag <- lag
  color <- color
  ccf(one, two, main = main, lag.max = lag, xlab = "", ylab = "", ci.col = color)
  #end run_ccf
}
one <- ts(exrates.df$returns.USD.EUR)
two <- ts(exrates.df$returns.USD.GBP)
# or
one <- exrates.zr[,1]
two <- exrates.zr[,2]
title <- "EUR vs. GBP"
run_ccf(abs(one), abs(two), main = title, lag = 20, color = "red")
# now for volatility (sizes)
one <- ts(abs(exrates.zr[,1]))
two <- ts(abs(exrates.zr[,2]))
title <- "EUR vs. GBP: volatility"
run_ccf(one, two, main = title, lag = 20, color = "red")
# We see some small raw correlations across time with raw returns. More revealing, we see volatility of correlation clustering using return sizes. 
```

One more experiment, rolling correlations and volatilities using these functions:	

```{r Rolling examples}
corr_rolling <- function(x) {	
  dim <- ncol(x)	
  corr_r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]	
  return(corr_r)	
}
vol_rolling <- function(x){
  library(matrixStats)
  vol_r <- colSds(x)
  return(vol_r)
}
ALL.r <- exrates.xts[, 1:4]
window <- 90 #reactive({input$window})
corr_r <- rollapply(ALL.r, 
                    width = window, 
                    corr_rolling, 
                    align = "right", 
                    by.column = FALSE)
colnames(corr_r) <- c("EUR.GBP", "EUR.CNY", "EUR.JPY", "GBP.CNY", "GBP.JPY", "CNY.JPY")
vol_r <- rollapply(ALL.r, 
                   width = window, 
                   vol_rolling, 
                   align = "right", 
                   by.column = FALSE)
colnames(vol_r) <- c("EUR.vol", "GBP.vol", "CNY.vol", "JPY.vol")
year <- format(index(corr_r), "%Y")
r_corr_vol <- merge(ALL.r, corr_r, vol_r, year)
```


4. How related are correlations and volatilities? Put another way, do we have to be concerned that inter-market transactions (e.g., customers and vendors transacting in more than one currency) can affect transactions in a single market? Let's  model the the `exrate` data to understand how correlations and volatilities depend upon one another.

```{r Related Correlations and Volatilities}
library(quantreg)
taus <- seq(.05,.95, .05)	# Roger Koenker UIC Bob Hogg and Allen Craig
fit.rq.CNY.JPY <- rq(log(CNY.JPY) ~ log(JPY.vol), tau = taus, data = r_corr_vol)	
fit.lm.CNY.JPY <- lm(log(CNY.JPY) ~ log(JPY.vol), data = r_corr_vol)	
# Some test statements	
CNY.JPY.summary <- summary(fit.rq.CNY.JPY, se = "boot")
CNY.JPY.summary
plot(CNY.JPY.summary)
# <TODO>

```

Here is the quantile regression part of the package.
	
1. We set `taus` as the quantiles of interest.	
2. We run the quantile regression using the `quantreg` package and a call to the `rq` function.	
3. We can overlay the quantile regression results onto the standard linear model regression.	
4. We can sensitize our analysis with the range of upper and lower bounds on the parameter estimates of the relationship between correlation and volatility.
5. The log()-log() transformation allows us to interpret the regression coefficients as elasticities, which vary with the quantile. The larger the elasticity, especially if the absolute value is greater than one, the more risk dependence one market has on the other.
6. The risk relationships can also be viewed year by year. Here we see very different patterns
7. $y = a + bx + e$ is interpreted as systematic movements in $y = a + bx$, while unsystematic movements are simply $e$.
 	
## Animation

```{r Animation}
library(quantreg)
library(magick)
img <- image_graph(res = 96)
datalist <- split(r_corr_vol, r_corr_vol$year)
out <- lapply(datalist, function(data){
  p <- ggplot(data, aes(JPY.vol, CNY.JPY)) +
    geom_point() + 
    ggtitle(data$year) + 
    geom_quantile(quantiles = c(0.05, 0.95)) + 
    geom_quantile(quantiles = 0.5, linetype = "longdash") +
    geom_density_2d(colour = "red")  
  print(p)
})
while (!is.null(dev.list()))  dev.off()
#img <- image_background(image_trim(img), 'white')
animation <- image_animate(img, fps = .5)
animation	
```
	
Attempt interpretations to help managers understand the way market interactions affect accounts receivables.

# Notes on lead and lag

In the `ccf()` function we get results that produce positive and negative lags. A positive lag looks back and a negative lag (a lead) looks forward in the history of a time series. Leading and lagging two different serries, then computing the moments and corelations show a definite asymmetry.

Suppose we lead the USD.EUR return by 5 days and lag the USD.GBP by 5 days. We will compare the correlation in this case with the opposite: lead the USD.GBP return by 5 days and lag the USD.EUR by 5 days.  We will use the `dplyr` package to help us.

```{r Table of Returns}
library(dplyr)
x <- as.numeric(exrates.df$returns.USD.EUR) # USD.EUR
y <- as.numeric(exrates.df$returns.USD.GBP) # USD.GBP
xy.df <- na.omit(data.frame(date = dates, ahead_x= lead(x, 5), behind_y = lag(y, 5)))
yx.df <- na.omit(data.frame(date = dates, ahead_y =lead(y, 5), behind_x = lag(x, 5)))
answer <- data_moments(na.omit(as.matrix(xy.df[,2:3])))
answer <- round(answer, 4)
knitr::kable(answer)
answer <- data_moments(na.omit(as.matrix(yx.df[,2:3])))
answer <- round(answer, 4)
knitr::kable(answer)
cor(as.numeric(xy.df$ahead_x), as.numeric(xy.df$behind_y))
cor(as.numeric(yx.df$ahead_y), as.numeric(yx.df$behind_x))
```

Leading x, lagging y will produce a negative correlation. The opposite produces an even smaller and positive correlation. Differences in means, etc. are not huge between the two cases, but when combined produce the correlational differences.

# Conclusion
## Skills and Tools
What methods & packages contributed towards data exploration and analytics?

The ability to interpret data moments, heteroscedasticity, autocorrelation, and partial autocorrelation are some essential components of the provided analysis.

The following methods & packages in R were used to explore the data:

1. dplyr
  + A grammar of data manipulation package
2. ggplot2
  + Create elegant data visualisations using the grammar of graphics package
3. magick
  + Advanced graphics and image-processing in R package
4. matrixStats
  + Functions that apply to rows and columns of matricies (and vectors) package
5. moments
  + Moments, cumulants, skewness, kurtosis and related tests package
6. quantreg
  + Quantile regression package
7. xts
  + eXtensible Time Series package
8. zoo
  + S3 infrastructure for regular and irregular time series package
  
The specific functions used where:  
  
1. na.omit
  + Returns the object with incomplete cases removed.
2. read.csv
  + Reads a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file. read.csv and read.csv2 are identical to read.table except for the defaults. They are intended for reading ‘comma separated value’ files (‘.csv’).
3. head
  + Returns the first or last parts of a vector, matrix, table, data frame or function. Since head() and tail() are generic functions, they may also have been extended to other classes.
4. tail
  + Returns the first (last) n rows when n >= 0 or all but the last (first) n rows when n < 0.
5. str
  + Compactly display the internal structure of an R object, a diagnostic function and an alternative to summary.
6. summary
  + Generic function used to produce result summaries of the results of various. model fitting functions. 
7. diff
  + Returns suitably lagged and iterated differences.
8. log
  + Computes logarithms, by default natural logarithms.
9. as.matrix
  + Attempts to turn its argument into a matrix.
10. as.Date
  + Convert an object to a date or date-time.
11. cbind
  + Take a sequence of vector, matrix or data-frame arguments and combine by columns or rows, respectively.
12. data.frame
  + Creates data frames, tightly coupled collections of variables which share many of the properties of matrices and of lists, used as the fundamental data structure by most of R's modeling software.
13. autoplot.zoo
  + Takes a zoo object and returns a ggplot2 object.
14. ggplotly
  + This function converts a ggplot2::ggplot() object to a plotly object.
15. acf
  + The function acf computes (and by default plots) estimates of the autocovariance or autocorrelation function.
16. pacf
  + Function used for the partial autocorrelations. 
17. colMeans
  + Calculates the mean for each column in a matrix.
18. colMedians
  + Calculates the median for each column in a matrix.
19. colSds
  + Standard deviation estimates for each column in a matrix.
20. colIQRs
  + Estimates of the interquartile range for each column in a matrix.
21. skewness
  + This function computes skewness of given data. In statistics, skewness is a measure of the asymmetry of the probability distribution of a random variable about its mean. In other words, skewness tells you the amount and direction of skew (departure from horizontal symmetry). The skewness value can be positive or negative, or even undefined.
22. kurtosis
  + This function computes the estimator of Pearson's measure of kurtosis. Like skewness, kurtosis is a statistical measure that is used to describe the distribution. Whereas skewness differentiates extreme values in one versus the other tail, kurtosis measures extreme values in either tail. Distributions with large kurtosis exhibit tail data exceeding the tails of the normal distribution (e.g., five or more standard deviations from the mean). Distributions with low kurtosis exhibit tail data that are generally less extreme than the tails of the normal distribution.
23. round
  + Round rounds the values in its first argument to the specified number of decimal places (default 0).
24. knitr::kable
  + This is a very simple table generator. It is simple by design. It is not intended to replace any other R packages for making tables.
25. mean
  + Generic function for the (trimmed) arithmetic mean.
26. quantile
  + The generic function quantile produces sample quantiles corresponding to the given probabilities. The smallest observation corresponds to a probability of 0 and the largest to a probability of 1.
27. paste
  + Concatenate vectors after converting to character.
28. ggplot
  + Initializes a ggplot object. It can be used to declare the input data frame for a graphic and to specify the set of plot aesthetics intended to be common throughout all subsequent layers unless specifically overridden.
29. aes
  + Aesthetic mappings describe how variables in the data are mapped to visual properties (aesthetics) of geoms.
30. stat_ecdf
  + The empirical cumulative distribution function (ECDF) provides an alternative visualisation of distribution.
31. geom_vline
  + These geoms add reference lines (sometimes called rules) to a plot, either horizontal, vertical, or diagonal (specified by slope and intercept). These are useful for annotating plots.
32. annotate
  + This function adds geoms to a plot, but unlike typical a geom function, the properties of the geoms are not mapped from variables of a data frame, but are instead passed in as vectors. This is useful for adding small annotations (such as text labels) or if you have your data in vectors, and for some reason don't want to put them in a data frame.
33. ts
  + Used to create time-series objects.
34. ccf
  + Computes the cross-correlation or cross-covariance of two univariate series.
35. abs
  + Computes the absolute value of x.
36. length
  + Get or set the length of vectors (including lists) and factors, and of any other R object for which a method has been defined.
37. ncol
  + Return the number of columns present in x.
38. cor
  + Compute the covariance of x and y if these are vectors.
39. rollapply
  + A generic function for applying a function to rolling margins of an array.
40. format
  + Format an R object for pretty printing.
41. index
  + Generic functions for extracting the index of an object and replacing it.
42. merge
  + Merge two data frames by common columns or row names, or do other versions of database join operations.
43. seq
  + Generate regular sequences. seq is a standard generic with a default method.
44. rq
  + Quantile regression.
45. lm
  + Used to fit linear models. It can be used to carry out regression, single stratum analysis of variance and analysis of covariance.
46. plot
  + Generic function for plotting of R objects.
47. image_graph
  + Graphics device that produces a Magick image. Can either be used like a regular device for making plots, or alternatively via image_draw to open a device which draws onto an existing image using pixel coordinates.
48. split
  + Divides the data in the vector x into the groups defined by f. 
49. lapply
  + Returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.
50. geom_point
  + The point geom is used to create scatterplots. The scatterplot is most useful for displaying the relationship between two continuous variables.
51. ggtitle
  + Title label on the plot.
52. geom_quantile
  + This fits a quantile regression to the data and draws the fitted quantiles with lines.
53. geom_density_2d
  + Perform a 2D kernel density estimation using MASS::kde2d() and display the results with contours. This can be useful for dealing with overplotting. This is a 2d version of geom_density().
54. image_animate
  + Operations to manipulate or combine multiple frames of an image.
55. as.numeric
  + Method should return an object of base type "numeric" number.
56. lag
  + Compute a lagged version of a time series, shifting the time base back by a given number of observations.

## Data Insights
Explain some of the data exploration, statistics and graphs here - include some research on foreign exchange rates.

## Business Remarks
Attempt interpretations to help managers understand they way market interactions affect accounts receivables.  What is the interplay between supplier costs and customer costs for this business?  How does China rates impact the situation?

